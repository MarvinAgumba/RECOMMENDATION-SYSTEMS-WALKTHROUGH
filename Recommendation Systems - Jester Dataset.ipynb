{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Recommendation Systems from Real-World Datasets\n",
    "\n",
    "## Jester Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset jester could not be found. Do you want to download it? [Y/n] Y\n",
      "Trying to download dataset from http://eigentaste.berkeley.edu/dataset/archive/jester_dataset_2.zip...\n",
      "Done! Dataset jester has been saved to C:\\Users\\msagu/.surprise_data/jester\n"
     ]
    }
   ],
   "source": [
    "#Load Dataset\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "jokes = Dataset.load_builtin(name=\"jester\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "surprise.dataset.DatasetAutoFolds"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test set\n",
    "trainset, testset = train_test_split(jokes, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice how there is no `X_train` or `y_train` in our values here. Our only features here are the ratings of other users and items, so we need to keep everything together. What is happening in the train-test split here is that `surprise` is randomly selecting certain $r_{ij}$ for users $u_{i}$ and items $i_{j} $. 80% of the ratings are in the training set and 20% in the test set. Let's investigate `trainset` and `testset` further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type trainset : <class 'surprise.trainset.Trainset'> \n",
      "\n",
      "Type testset : <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Type trainset :\", type(trainset), \"\\n\")\n",
    "print(\"Type testset :\", type(testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Interestingly enough, the values here are of different data types! The `trainset` is still a `surprise` specific data type that is optimized for computational efficiency and the `testset` is a standard Python list - you'll see why when we start making predictions. Let's take a look at how large our `testset` is as well as what's contained in an individual element. A sacrifice of `surprise`'s implementation is that we lose a lot of the exploratory methods that are present with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352288\n",
      "('30253', '18', 2.844)\n"
     ]
    }
   ],
   "source": [
    "print(len(testset))\n",
    "print(testset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we can calculate the more simple **Memory-Based** , `neighborhood-based approaches. Some things to keep in mind are what type of similarities you should use. These can all have fairly substantial effects on the overall performance of the model. You'll notice that the API of surprise is very similar to scikit-learn when it comes to model fitting and testing. To begin with, we'll import the modules we'll be using for the neighborhood-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.prediction_algorithms import knns\n",
    "from surprise.similarities import cosine, msd, pearson\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One of our first decisions is item-item similarity versus user-user similarity. For the sake of computation time, it's best to calculate the similarity between whichever number is fewer, users or items. Let's see what the case is for our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users:  58771 \n",
      "\n",
      "Number of items:  140 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of users: \", trainset.n_users, \"\\n\")\n",
    "print(\"Number of items: \", trainset.n_items, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are clearly way more users than items! We'll take that into account when inputting the specifications to our similarity metrics. Because we have fewer items than users, it will be more efficient to calculate item-item similarity rather than user-user similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_cos = {\"name\": \"cosine\", \"user_based\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train our model. Note that if you decide to train this model with `user_based=True`, it will take quite some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.knns.KNNBasic at 0x2179c54ffa0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic = knns.KNNBasic(sim_options=sim_cos)\n",
    "basic.fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now our model is fit! Let's take a look at the similarity metrics of each of the items to one another by using the `sim` attribute of our fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.47953819, 0.43737462, ..., 0.2756085 , 0.59669782,\n",
       "        0.54984451],\n",
       "       [0.47953819, 1.        , 0.41375342, ..., 0.09369038, 0.56795701,\n",
       "        0.23988625],\n",
       "       [0.43737462, 0.41375342, 1.        , ..., 0.05113336, 0.45871563,\n",
       "        0.33187442],\n",
       "       ...,\n",
       "       [0.2756085 , 0.09369038, 0.05113336, ..., 1.        , 0.1363809 ,\n",
       "        0.47213899],\n",
       "       [0.59669782, 0.56795701, 0.45871563, ..., 0.1363809 , 1.        ,\n",
       "        0.19791064],\n",
       "       [0.54984451, 0.23988625, 0.33187442, ..., 0.47213899, 0.19791064,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic.sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to test the model to determine how well it performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = basic.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.1941\n",
      "4.194079161054342\n"
     ]
    }
   ],
   "source": [
    "print(accuracy.rmse(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Not a particularly amazing model.... As you can see, the model had an RMSE of about 4.19, meaning that it was off by roughly 4 points for each guess it made for ratings. Not horrendous when you consider we're working on a range of 20 points, but let's see if we can improve it. To begin with, let's try with a different similarity metric (Pearson correlation) and evaluate our RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 4.2565\n",
      "4.256507954112282\n"
     ]
    }
   ],
   "source": [
    "sim_pearson = {\"name\": \"pearson\", \"user_based\": False}\n",
    "basic_pearson = knns.KNNBasic(sim_options=sim_pearson)\n",
    "basic_pearson.fit(trainset)\n",
    "predictions = basic_pearson.test(testset)\n",
    "print(accuracy.rmse(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pearson correlation seems to have performed better than cosine similarity in the basic KNN model, we can go ahead and use Pearson correlation as our similarity metric of choice for future models. The next model we're going to try is [KNN with Means](https://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNWithMeans). This is the same thing as the basic KNN model, except it takes into account the mean rating of each user or item depending on whether you are performing user-user or item-item similarities, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 4.1149\n",
      "4.114948186536157\n"
     ]
    }
   ],
   "source": [
    "sim_pearson = {\"name\": \"pearson\", \"user_based\": False}\n",
    "knn_means = knns.KNNWithMeans(sim_options=sim_pearson)\n",
    "knn_means.fit(trainset)\n",
    "predictions = knn_means.test(testset)\n",
    "print(accuracy.rmse(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A little better... let's try one more neighborhood-based method before moving into model-based methods. Let's try the [KNNBaseline](https://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNBaseline) method. This is a more advanced method because it adds in a bias term that is calculated by way of minimizing a cost function represented by:\n",
    "\n",
    "$$ \\sum_{r_{ui} \\in R_{\\text{train}}}{(\\hat{r}_{ui} - ( \\mu + b_{i} + b_{u}))^{2} + \\lambda(b_u^2 + b_i^2) } $$\n",
    "\n",
    "With $b_i$ and $b_u$ being biases for items and users respectively and $\\mu$ referring to the global mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 4.1108\n",
      "4.110825720301996\n"
     ]
    }
   ],
   "source": [
    "sim_pearson = {\"name\": \"pearson\", \"user_based\": False}\n",
    "knn_baseline = knns.KNNBaseline(sim_options=sim_pearson)\n",
    "knn_baseline.fit(trainset)\n",
    "predictions = knn_baseline.test(testset)\n",
    "print(accuracy.rmse(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Even better! Now let's see if we can get some insight by applying some matrix factorization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model-Based Methods (Matrix Factorization)**: It's worth pointing out that when SVD is calculated for recommendation systems, it is preferred to be done with a modified version called \"Funk's SVD\" that only takes into account the rated values, ignoring whatever items have not been rated by users. The algorithm is named after Simon Funk, who was part of the team who placed 3rd in the Netflix challenge with this innovative way of performing matrix decomposition. Read more about Funk's SVD implementation at [his original blog post](https://sifter.org/~simon/journal/20061211.html). There is no simple way to include for this fact with SciPy's implementation of `svd()`, but luckily the `surprise` library has Funk's version of SVD implemented to make our lives easier!\n",
    "\n",
    "Similar to other `sklearn` features, we can expedite the process of trying out different parameters by using an implementation of grid search. Let's make use of the grid search here to account for some different configurations of parameters within the SVD pipeline. This might take some time! You'll notice that the `n_jobs` is parameter set to -1, which ensures that all of the cores on your computer will be used to process fitting and evaluating all of these models. To help keep track of what is occurring here, take note of the different values. This code ended up taking over 16 minutes to complete even with parallelization in effect, so the optimal parameters are given to you for the SVD model below. Use them to train a model and let's see how well it performs. If you want the full grid search experience, feel free to uncomment the code and give it a go!\n",
    "\n",
    "The optimal parameters are :\n",
    "\n",
    "```python\n",
    "{'n_factors': 100, 'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.4}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.prediction_algorithms import SVD\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = {'n_factors':[20, 100],'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],\n",
    "#               'reg_all': [0.4, 0.6]}\n",
    "# gs_model = GridSearchCV(SVD,param_grid=param_grid,n_jobs = -1,joblib_verbose=5)\n",
    "# gs_model.fit(jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.2655\n",
      "4.2654506485692645\n"
     ]
    }
   ],
   "source": [
    "svd = SVD(n_factors=100, n_epochs=10, lr_all=0.005, reg_all=0.4)\n",
    "svd.fit(trainset)\n",
    "predictions = svd.test(testset)\n",
    "print(accuracy.rmse(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Interestingly, this model performed worse than the others! In general, the advantages of matrix factorization starts to show itself when the size of the dataset becomes massive. At that point, the storage challenges increase for the memory-based models, and there is enough data for latent factors to become extremely apparent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making Predictions:** Now that we've explored some models, we can think about how we might fit the models into the context of an application. To begin with, let's access some basic functionality of `surprise` models to get predicted ratings for a given user and item. All that's needed are the `user_id` and `item_id` for which you want to make a prediction. Here we're making a prediction for user 34 and item 25 using the SVD model we just fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(uid='34', iid='25', r_ui=None, est=2.7186659134472038, details={'was_impossible': False})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_34_prediction = svd.predict(\"34\", \"25\")\n",
    "user_34_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the prediction is a tuple. Here, we're going to access the estimated rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7186659134472038"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_34_prediction[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You might be wondering, \"OK I'm making predictions about certain items rated by certain users, but how can I actually give certain N recommendations to an individual user?\" Although `surprise`  is a great library, it does not have this recommendation functionality built into it, but in the next illustration, you will get some experience not only fitting recommendation system models, but also programmatically retrieving recommended items for each user."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
